<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yamada - AI & LLM Tutor</title>
    <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-50 font-sans">
    <!-- Sticky Navigation -->
    <nav class="bg-blue-600 text-white sticky top-0 z-10 shadow-md">
        <div class="container mx-auto px-4 py-4 flex justify-between items-center">
            <h1 class="text-2xl font-bold">AI & LLM Tutor</h1>
            <ul class="flex space-x-6">
                <li><a href="#about" class="hover:text-gray-200">About</a></li>
                <li><a href="#qualifications" class="hover:text-gray-200">Qualifications</a></li>
                <li><a href="#courses" class="hover:text-gray-200">Courses</a></li>
                <li><a href="#projects" class="hover:text-gray-200">Projects</a></li>
                <li><a href="#training" class="hover:text-gray-200">Training</a></li>
            </ul>
        </div>
    </nav>

    <!-- Hero Section -->
    <section class="bg-blue-100 py-16">
        <div class="container mx-auto px-4 text-center">
            <h1 class="text-4xl md:text-5xl font-bold text-gray-800 mb-4">Expert AI & LLM Tutoring</h1>
            <p class="text-xl text-gray-600 mb-6">By Yamada</p>
            <p class="text-lg text-gray-600 max-w-2xl mx-auto">
                With over 8 years of AI expertise and 4 years of teaching, I specialize in tutoring students and professionals in large language models, NLP, and AI solutions, empowering learners with practical, industry-relevant skills.
            </p>
        </div>
    </section>

    <!-- Main Content -->
    <main class="container mx-auto px-4 py-12">
        <!-- About Section -->
        <section id="about" class="mb-16">
            <h2 class="text-3xl font-semibold text-gray-800 mb-6 text-center">Introduction</h2>
            <div class="bg-white shadow-md rounded-lg p-6 max-w-3xl mx-auto">
                <p class="text-gray-600">
                    As a dedicated tutor, Yamada brings 8 years of hands-on technical experience in AI, specializing in large language models (LLMs), NLP, and enterprise solutions. His career spans roles as a co-founder, technical lead, and NLP algorithm engineer, delivering successful AI projects in digitalization, information security, and e-commerce retail.<br><br>
                    With 4 years of training experience, Yamada has taught over 1,000 students in LLM and AI-related courses. His engaging, rigorous teaching style combines case studies, practical exercises, and real-world insights, empowering learners from beginners to experts to master large language models and AI technologies.
                </p>
            </div>
        </section>

        <!-- Qualifications Section -->
        <section id="qualifications" class="mb-16">
            <h2 class="text-3xl font-semibold text-gray-800 mb-6 text-center">Qualifications</h2>
            <div class="bg-white shadow-md rounded-lg p-6 max-w-3xl mx-auto">
                <ul class="list-disc pl-6 text-gray-600">
                    <li>Senior Knowledge Graph Engineer at Ctrip</li>
                    <li>Senior NLP Algorithm Engineer at Baidu</li>
                    <li>Co-founder of a Novel Large-Scale Model Startup</li>
                    <li>Technical Lead at a Coding Agent Startup</li>
                    <li>8+ years of technical experience</li>
                    <li>4+ years of training experience</li>
                </ul>
            </div>
        </section>

        <!-- Courses Section -->
        <section id="courses" class="mb-16">
            <h2 class="text-3xl font-semibold text-gray-800 mb-6 text-center">Courses</h2>
            <div class="bg-white shadow-md rounded-lg p-6 max-w-3xl mx-auto">
                <ul class="list-disc pl-6 text-gray-600">
                    <li>Best Practices for Code Auto-Generation Agents Based on MCP-Agents</li>
                    <li>Multi-Agent Collaboration and Evaluation Optimization</li>
                    <li>Best Practices for Local Knowledge Q&A Systems with DeepSeek + LangChain</li>
                    <li>AI Open-Source Large Model Training and Fine-Tuning</li>
                    <li>Large Model Fine-Tuning Course</li>
                    <li>DeepSeek-Powered Efficiency Improvement</li>
                    <li>Pre-Trained Model Applications</li>
                    <li>Knowledge Graph Application Development</li>
                </ul>
            </div>
        </section>

        <!-- Project Experience Section -->
        <section id="projects" class="mb-16">
            <h2 class="text-3xl font-semibold text-gray-800 mb-6 text-center">Project Experience</h2>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h3 class="text-xl font-semibold text-gray-700 mb-2">Coding Agent Startup: Code Auto-Generation Platform</h3>
                    <p class="text-gray-600">
                        <strong>Project Description:</strong> Developed a coding agent platform, including scheme design, prompt optimization, and agent framework design.<br>
                        <strong>Responsibilities:</strong> Architecture design, scheme design and implementation, product development, and testing.<br>
                        The platform features a dialogue system, document generation, and code auto-generation. It uses OpenHands’ CodeAct module to read repositories, generate documents (BRD/PRD/UML/Backlog), and submit PRs to GitHub via MCP-Agent and RAG systems.
                    </p>
                </div>
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h3 class="text-xl font-semibold text-gray-700 mb-2">SQL Auto-Generation Large Model Platform</h3>
                    <p class="text-gray-600">
                        <strong>Project Description:</strong> Built for an e-commerce startup to auto-generate SQL queries.<br>
                        <strong>Responsibilities:</strong> Scheme design, model training, prompt engineering, model deployment.<br>
                        Used CodeLlama34b with LoRA fine-tuning, Spider dataset, and TGI/vLLM deployment for routing, schema link generation, and SQL output.
                    </p>
                </div>
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h3 class="text-xl font-semibold text-gray-700 mb-2">Novel Large Model Platform</h3>
                    <p class="text-gray-600">
                        <strong>Project Description:</strong> Replaced traditional novel writing with large model capabilities.<br>
                        <strong>Responsibilities:</strong> Architecture design, model training, deployment, prompt engineering.<br>
                        Fine-tuned WizardLM-Uncensored-SuperCOT-StoryTelling-30B with LoRA, using vector databases for user-driven novel rewriting.
                    </p>
                </div>
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h3 class="text-xl font-semibold text-gray-700 mb-2">Procter & Gamble E-Commerce Knowledge Graph Platform</h3>
                    <p class="text-gray-600">
                        <strong>Project Description:</strong> Built a knowledge graph for e-commerce data to enhance recommendations.<br>
                        <strong>Responsibilities:</strong> Scheme design, model design, training, deployment.<br>
                        Used Cosent for entity alignment and BERT with LMCL loss for relation extraction.
                    </p>
                </div>
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h3 class="text-xl font-semibold text-gray-700 mb-2">Baidu Legal Domain Dialogue System</h3>
                    <p class="text-gray-600">
                        <strong>Project Description:</strong> Developed a dialogue system for legal consultations.<br>
                        <strong>Responsibilities:</strong> Scheme design, model training, deployment.<br>
                        Used ERNIE-based models for intent and slot recognition, enhanced by ADEA+MLM and s-BERT+R-Drop.
                    </p>
                </div>
            </div>
        </section>

        <!-- Training Experience Section -->
        <section id="training" class="mb-16">
            <h2 class="text-3xl font-semibold text-gray-800 mb-6 text-center">Training Experience</h2>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h3 class="text-xl font-semibold text-gray-700 mb-2">Shanghai Telecom: Graph Database and Knowledge Graph</h3>
                    <p class="text-gray-600">
                        <strong>Content:</strong><br>
                        · Systematic explanation of Neo4j core concepts and usage.<br>
                        · Mastery of Cypher query language (basic and advanced, including multi-hop and subgraph queries).<br>
                        · In-depth entity extraction (BERT+BiLSTM+CRF), relation extraction (BERT+TextCNN), and entity alignment (Cosent).<br>
                        · Graph structure design and visualization methods.<br>
                        <strong>Teaching Method:</strong> Combined theory, case studies, code practice, and project setup, guiding learners to build a business-relevant knowledge graph system.<br>
                        <strong>Outcomes:</strong> Learners independently completed triplet extraction, Neo4j deployment, and graph visualization, applying knowledge graphs to telecom data.<br>
                        <strong>Feedback:</strong> Praised for strong practicality, job relevance, and tight integration of technology and business.
                    </p>
                </div>
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h3 class="text-xl font-semibold text-gray-700 mb-2">Fujian Telecom: Enterprise Large Model Applications</h3>
                    <p class="text-gray-600">
                        <strong>Focus:</strong> Deployment, fine-tuning, and service-oriented applications of open-source large models, integrating agents with business to improve conversion rates.<br>
                        <strong>Key Knowledge Points:</strong><br>
                        · Enterprise RAG System Design and Deployment: Vector databases, embedding models, generative models, retrieval models, and cmrc2018-based RAG practice.<br>
                        · Agent Design and Deployment: Core concepts (Tool, Memory, Plan), MCP and Swarm frameworks, and MCP-Agent-based practice.<br>
                        · Large Model Fine-Tuning: Data preparation, LoRA vs. full fine-tuning, and telecom data-based Alpaca-LoRA practice.<br>
                        · Model Distillation and Quantization: Strategies, model selection, deployment optimization, and Qwen-based distillation practice.<br>
                        <strong>Practical Features:</strong> Task-driven, covering data preparation, model loading, tuning, development, and deployment.<br>
                        <strong>Outcomes:</strong> Learners applied skills in pilot projects, enhancing AI integration in telecom business.
                    </p>
                </div>
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h3 class="text-xl font-semibold text-gray-700 mb-2">Hubei Telecom: AI Talent Development</h3>
                    <p class="text-gray-600">
                        <strong>Focus:</strong><br>
                        · Computer Vision Basics: Tasks, trends, and commercial applications.<br>
                        · Image Classification: CNN-based, self-attention-based, and domain-specific models.<br>
                        · Object Detection: Two-stage, single-stage, and YOLO models.<br>
                        · Image Segmentation: CNN-based, self-attention-based, DeepLabv3+, and Segment Anything models.<br>
                        · Image Generation: DCGAN model and applications.<br>
                        · NLP Basics: Overview, word embeddings, RNNs, language models, Transformer.<br>
                        · NLP Tasks: Text classification (TextCNN), machine translation (Seq2seq, Attention, Transformer-based).<br>
                        · Knowledge Graphs and Q&A: Entity recognition, relation extraction, and intelligent Q&A.<br>
                        · AI Large Model Theory: Transformer structure, attention, context windows, and efficiency.<br>
                        · Mainstream Models: GPT, Llama, GLM, DeepSeek, Vision Transformer, SAM.<br>
                        · Lightweight Deployment: ONNX, Triton, and cloud deployment.<br>
                        <strong>Practical Features:</strong> Covered model understanding to system building, with practical modules for multiple AI tasks.<br>
                        <strong>Outcomes:</strong> Learners gained comprehensive AI understanding, transitioning from model use to tuning and deployment.
                    </p>
                </div>
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h3 class="text-xl font-semibold text-gray-700 mb-2">Jiangxi Telecom: Prompt Engineering</h3>
                    <p class="text-gray-600">
                        <strong>Focus:</strong><br>
                        · Prompt Engineering Concepts: Model mechanisms, telecom applications (customer service, ticketing, fault diagnosis), design principles, and advanced techniques.<br>
                        · Prompt Case Studies: Ticket element extraction, multi-turn dialogue, output constraints, and intelligent customer service systems.<br>
                        · RAG Paradigm: Definition, workflow, and challenges in data extraction, indexing, retrieval, and generation.<br>
                        <strong>Practical Features:</strong> Hands-on prompt management, template handling, history tracking, and parameter tuning.<br>
                        <strong>Outcomes:</strong> Learners designed industry-specific prompt templates, improving human-machine interaction quality.
                    </p>
                </div>
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h3 class="text-xl font-semibold text-gray-700 mb-2">Beijing Telecom: Multimodal Large Models</h3>
                    <p class="text-gray-600">
                        <strong>Focus:</strong><br>
                        · Model Evaluation: Metrics, basic and advanced ability testing, and comprehensive evaluation systems.<br>
                        · Multimodal Basics: Definition and core elements.<br>
                        · Multimodal Models: CLIP, alignment techniques, and open-source model usage.<br>
                        · Applications: Voice assistants, virtual avatars, text-to-image (Stable Diffusion, DALL-E), document parsing, medical imaging, robotics, and teaching tools.<br>
                        · Practical Training: Multimodal fine-tuning, data processing, LoRA, and distributed training.<br>
                        <strong>Practical Features:</strong> Business-driven tasks like image-text Q&A and speech recognition, covering data building to deployment.<br>
                        <strong>Outcomes:</strong> Learners mastered multimodal model usage, configuration, and tasks like image-text generation.
                    </p>
                </div>
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h3 class="text-xl font-semibold text-gray-700 mb-2">DeepEye: Pre-Trained Model Principles and Practice</h3>
                    <p class="text-gray-600">
                        <strong>Focus:</strong><br>
                        · Transformer Principles: Encoder-decoder, self-attention, multi-head attention, layer normalization, mask mechanisms.<br>
                        · Model Comparison: ELMo (bidirectional BiLSTM), GPT (unidirectional Transformer decoder), BERT (bidirectional Transformer encoder), ALBERT (lightweight).<br>
                        · Long-Text Models: Transformer-XL, XL-Net, and positional encoding.<br>
                        <strong>Teaching Format:</strong> Code practice, Q&A, quizzes, and practical tasks.<br>
                        <strong>Practical Cases:</strong> Learners completed projects in text classification, intent recognition, and Q&A systems.<br>
                        <strong>Teaching Highlights:</strong><br>
                        ➢ Bilibili live Q&A for extended learning.<br>
                        ➢ Daily WeChat group support for technical queries.<br>
                        ➢ Project guidance from requirement clarification to evaluation.
                    </p>
                </div>
            </div>
        </section>
    </main>

    <!-- Footer -->
    <footer class="bg-gray-800 text-white py-6">
        <div class="container mx-auto px-4 text-center">
            <p>Contact: <a href="mailto:zqx0310liubo@gmail.com" class="underline hover:text-gray-200">zqx0310liubo@gmail.com</a> | © 2025 Yamada</p>
        </div>
    </footer>
</body>
</html>